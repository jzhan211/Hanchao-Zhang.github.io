[{"authors":["admin"],"categories":null,"content":"Hanchao Zhang is a second year Ph.D. student in the Biostatistics Division at Department of Population Health, Sackler institute of Biomedical Sciences, Grossman School of Medicine, New York University.\nHanchao Zhang is a alumnus of Open Case Study Project at the Bloomberg School of Public Health  of the Johns Hopkins University\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://hanchao-zhang.github.io/author/hanchao-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hanchao-zhang/","section":"authors","summary":"Hanchao Zhang is a second year Ph.D. student in the Biostatistics Division at Department of Population Health, Sackler institute of Biomedical Sciences, Grossman School of Medicine, New York University.\nHanchao Zhang is a alumnus of Open Case Study Project at the Bloomberg School of Public Health  of the Johns Hopkins University","tags":null,"title":"Hanchao Zhang","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://hanchao-zhang.github.io/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Overview It\u0026rsquo;s a first semester course for statistical inference I.\nReference Books Includes: Statistical Inference Second Edition, george casella roger l. berger\nResources Chapter 1 Measure Theory and Probability Theory Notes\nChapter 2 Transformations and Expectations\nChapter 3 Common Families of Distributions\nChapter 4 Multiple random Variables\n","date":1567987200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567987200,"objectID":"8984af45d423f5e06a2976305f570b35","permalink":"https://hanchao-zhang.github.io/courses/statistical-inference-i/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/courses/statistical-inference-i/","section":"courses","summary":"Probability Theory, Transformations and Expectations, Common Families of Distributions, and Multiple random Variables","tags":null,"title":"Statistical Inference I","type":"docs"},{"authors":null,"categories":null,"content":"Overview It\u0026rsquo;s a second semester course for statistical inference II.\nReference Books Includes: Statistical Inference Second Edition, george casella roger l. berger\nResources Chapter 5 Properties of a Random Sample Notes\nChapter 6 Principles of Data Reduction Notes\nChapter 7 Point Estimation\nChapter 8 Hypothesis Testing\n","date":1578528000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1578528000,"objectID":"154e648f3da53c5dc60febee81f0b819","permalink":"https://hanchao-zhang.github.io/courses/statistical-inference-ii/","publishdate":"2020-01-09T00:00:00Z","relpermalink":"/courses/statistical-inference-ii/","section":"courses","summary":"Properties of a Random Sample, Principles of Data Reduction, Point Estimation, and Hypothesis Testing","tags":null,"title":"Statistical Inference II","type":"docs"},{"authors":null,"categories":null,"content":"Overview Probability, Descriptive Statistics, Hypothesis Testing, and Linear Model\n","date":1567987200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1283990400,"objectID":"f50733727f4c510fd6c877192b0e08b6","permalink":"https://hanchao-zhang.github.io/courses/biostatistics-i/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/courses/biostatistics-i/","section":"courses","summary":"Probability, Descriptive Statistics, Hypothesis Testing, and Linear Model","tags":null,"title":"Biostatistics I","type":"docs"},{"authors":null,"categories":null,"content":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2979f97b365651b03038f8cf60055f8a","permalink":"https://hanchao-zhang.github.io/others/hero/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/hero/","section":"others","summary":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar","tags":null,"title":"Academic","type":"others"},{"authors":null,"categories":null,"content":"Welcome to the personal demo of Academic. Other demos available include:\n  Project Demo (Academic\u0026rsquo;s actual site)  Over 100,000 Amazing Websites have Already Been Built with Academic\n Join the Most Empowered Hugo Community\n This homepage section is an example of adding elements to the Blank widget.\nBackgrounds can be applied to any section. Here, the background option is set give an image parallax effect.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53a88ab6b704a144e8e867f9a16a14f1","permalink":"https://hanchao-zhang.github.io/others/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/home/","section":"others","summary":"Welcome to the personal demo of Academic. Other demos available include:\n  Project Demo (Academic\u0026rsquo;s actual site)  Over 100,000 Amazing Websites have Already Been Built with Academic\n Join the Most Empowered Hugo Community","tags":null,"title":"Demos","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a1bb70137587d49cc731dbbcf18d56ca","permalink":"https://hanchao-zhang.github.io/others/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/skills/","section":"others","summary":"","tags":null,"title":"Skills","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f29db5639de567f277ad9942e0eded96","permalink":"https://hanchao-zhang.github.io/others/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/experience/","section":"others","summary":"","tags":null,"title":"Education","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ef687711ba1b591ecab01ffb613a874","permalink":"https://hanchao-zhang.github.io/others/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/accomplishments/","section":"others","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4b1865cf554cd7007d14f3cde82513af","permalink":"https://hanchao-zhang.github.io/others/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/posts/","section":"others","summary":"","tags":null,"title":"Recent Posts","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"574789e344ebc1a295790d7423e04708","permalink":"https://hanchao-zhang.github.io/others/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/projects/","section":"others","summary":"","tags":null,"title":"Projects","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0ac5f33e991ab6ab5a4ebf11b0e40964","permalink":"https://hanchao-zhang.github.io/others/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/people/","section":"others","summary":"","tags":null,"title":"Meet the Team","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65f76afb535570a0906992817027d2eb","permalink":"https://hanchao-zhang.github.io/others/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/talks/","section":"others","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7ba4734003827a8aee1de1ef8a227c8c","permalink":"https://hanchao-zhang.github.io/others/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/featured/","section":"others","summary":"","tags":null,"title":"Featured Publications","type":"others"},{"authors":null,"categories":null,"content":" Quickly discover relevant content by filtering publications.   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"733e739eecfe93d504cbcbd1aa27b348","permalink":"https://hanchao-zhang.github.io/others/publications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/publications/","section":"others","summary":" Quickly discover relevant content by filtering publications.   ","tags":null,"title":"Recent Publications","type":"others"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"66f42554d08f03f46489613485da4be8","permalink":"https://hanchao-zhang.github.io/others/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/others/tags/","section":"others","summary":"","tags":null,"title":"Popular Topics","type":"others"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://hanchao-zhang.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"1. Introduction The effects of outliers in the functional data analysis are well aware by statisticians in recent years, there have been some works of literature on finding outliers in the functional data. However, most of the literature focused on the detection of the extreme values in the functional data in a univariate case. For example, Manuel Febrero Environmetrics 2008; 19: 331-345 were focusing on the detection of extreme values that is abnormally large or small compared with the rest of the values. Ana Arribas-Gil Biostatistics, Volume 15, Issue 4, October 2014 were focusing on the detection of shape outliers (defined by different shape from the rest of the sample). Our method of outlier detection and imputation finds a way to detect the outlier due to the misfunction of the medical devices or detaching of the devices from the patients, which not necessarily produce extreme values or differences in shapes, but a graduate changes of the trend.\nOur method was inspired by real data acquired from the Intensive Care Unite. The data was generated from the medical device that attached to the patients\u0026rsquo; hand or head and measured the oxygen level and carbon dioxide level during the CPR procedure.\n2. Objective We proposed a voting method to detect different types of outliers in the real data. We then evaluate the algorithm using some simulated data.\n3. Types of the outliers in the real data Misfunction and detaching of the devices during the CPR may produce different kinds of outliers in terms of their shape, values, and trend. We attached some of the typical examples of the outliers in the real data.\n3.1 Consecutive jumps during the measuring time knitr::include_graphics(\u0026quot;o1.png\u0026quot;) knitr::include_graphics(\u0026quot;o2.png\u0026quot;)  This kind of jump in the data is not the most common one but the one that is easiest to solve. Since there are only a few points that lie away from the trend of the data, we could simply remove it by fitting a non-parametric regression using kernel and splines, and then remove the points that at preset times away from the $\\mu$ at that time point.\n3.2 Nonconsecutive jumps during the measuring time knitr::include_graphics(\u0026quot;o3.png\u0026quot;) knitr::include_graphics(\u0026quot;o4.png\u0026quot;)  There are two examples of nonconsecutive jumps during the measuring time, which are more common ones in the real data. Both curves on the left and right show clear patterns of the trend. Even though the number of outliers is more than the first scenario, the general trends of the data were not corrupted by the outliers. The outliers can be found and detected by fitting smooth curve, such as a none-parametric spline with a small number of the knots, and applying the previous standard to remove the outliers\n3.3 Jumps that corrupt the trend of the data knitr::include_graphics(\u0026quot;o5.png\u0026quot;) knitr::include_graphics(\u0026quot;o6.png\u0026quot;)  These kinds of jumps are the ones that hard to deal with. First of all, the true trend is hard to identify. Moreover, the observed values were all pretty far away from some expected value after fitting a smooth function.\n3.4 Unexpected jumps due to a large number of outliers knitr::include_graphics(\u0026quot;o7.png\u0026quot;) knitr::include_graphics(\u0026quot;o8.png\u0026quot;)  The two examples above show one type of unexpected jumps; some extreme values generated the jumps. These outliers are easy to detect using the window screening method (one of the models in the voting methods that we proposed). The voting method is similar to a k fold cross-validation. However, the validation set is not randomly selected but a preset number of consecutive points in the data.\nknitr::include_graphics(\u0026quot;o9.png\u0026quot;) knitr::include_graphics(\u0026quot;o10.png\u0026quot;)  These two examples of the unexpected jumps due to a large number of outliers is different from the previous ones. The outliers that cause the jump are not extreme values but the value that gradually deviates away from the general trend. The left one has two unexpected peaks around the x-axis at - 1500 and -500. The right one has two unexpected peaks around the x-axis at - 400 and - 200.\n4. Voting methods to detect and impute the outliers 4.1 Non-parametric splines (1st model in the voting methods) These are the methods that we can use to remove the outliers in the first two scenarios.\n  Fit a non-parametric spline with an arbitrary number of knots using the whole dataset $\\mathcal{D}$\n  Obtain the fitted value $y^*$ and the standard deviation $sd$ at each time points\n  Index the outliers $\\text{I}^{np}_k = ( |\\hat y_k - y_k^*| \u0026gt; n \\times sd_k)$, $n$ is a arbitrary number, where $\\hat y$ belongs to the observed dataset $\\mathcal{D}$ and $k = 1,2,3, \\cdots n$, and $n$ is the number of the observation in the dataset $\\mathcal{D}$\n  Calculate the reduced sum of the squares $MSE^{np} = \\sum_{k=1}^n (\\hat y - y^*)^2 \\times I_k^{np}$\n  4.2 Window process (2nd model in the voting methods)   Create a vector $W$ that represents the length of windows that will be used in the window precess, $0 \u0026lt; |W| \u0026lt; n$(where n is the total observation in the data)\n  Set the window length to $W_i$ ( where $0 \u0026lt; i \\le |W|$ ) and start a cross-validation like process\n2.0. create a vector $I_i^{wp}$ with all 0 in the vector, such that $|I_i| = n$\n2.1. removed the set of observation $\\mathcal{A}$ from the whole dataset $\\mathcal{D}$ and denote it as $\\mathcal{D}{ij}$, and denote the complementary set of $\\mathcal{D}{ij}$ as $\\mathcal{D}{ij}^c$, where $\\mathcal{A} = {\\omega:\\omega \\in[x_j,x{j + W_i}] , x_k\\in\\mathcal{D}}$, and $k = 1,2,3, \\cdots , n$\n2.2. fit a non-parametric spline using $\\mathcal{D}{ij}$, denote it as $\\mathcal{M}{ij}$\n2.3. predit the data $\\mathcal{D}{ij}^c$ using $\\mathcal{M}{ij}$, denote the predicted dataset as $\\widetilde{\\mathcal{D}_{ij}^c}$\n2.4. let $\\hat y_k$ and $\\tilde y_k$ are eletments in $\\mathcal{D}{ij}$ and $\\widetilde{\\mathcal{D}{ij}^c}$ respectively, let $sd_k$ represent the standard deviation for $\\widetilde y_k$ and $I_{ij}^{wp}=I_{ij}^{wp}( |\\hat y_k - \\tilde y_k| \u0026gt; n \\times sd_k )$, where $k = 1,2,3, \\cdots,j$\n2.5 repeat the step 2.1 to 2.4 until the end of the window approach the last observation\n  calculate the reduced sum of squares $MSE^{wp}i = \\sum{k=1}^j(\\hat y_k - \\tilde y_k)^2 I_{ik}$ where $k = 1,2,3, \\cdots, j$\n  Go back to step one and repeat for all preset window length and obtain the $MSE_i^{wp}$ and $I_i^{wp}$ for each window length $W_i \\subset W$\n  4.3 Voting MSE of from the two models above The voting method is a combined method of the two previous models. By using the voting method, we can get the best parameter by grid searching, which was arbitrary in the previous step.\nFor each length of the windows, we can compute a total reduced $MSE$ contructed by $MSE_1$, $MSE_2$, and $MSE_3$\n  We denote $\\text{MSE}{i1} = \\frac{1}{2} \\times ( \\frac{ \\sum{k=1}^n (\\hat y_k - y_k^*)^2 \\times I_k^{np} \\times I_{ik}^{wp} }{ \\sum_{k=1}^n I_k^{np} \\times I_{ik}^{wp}} + \\frac{ \\sum_{k=1}^n (\\hat y_k - \\tilde y_k)^2 \\times I_k^{np} \\times I_{ik}^{wp} }{ \\sum_{k=1}^n I_k^{np} \\times I_{ik}^{wp}})$\n  Denote $\\text{MSE}{i2} = \\frac{ \\sum{k=1}^n (\\hat y_k - y_k^*)^2 \\times I_k^{np} \\times (1-I_{ik}^{wp}) }{ \\sum_{k=1}^n I_k^{np} \\times (1-I_{ik}^{wp})}$\n  Denote $\\text{MSE}{i3} = \\frac{ \\sum{k=1}^n (\\hat y_k - \\tilde y_k)^2 \\times (1-I_k^{np}) \\times I_{ik}^{wp} }{ \\sum_{k=1}^n (1-I_k^{np}) \\times I_{ik}^{wp}}$\n  Calculate $\\text{MSE}i = \\text{MSE}{i1} - \\text{MSE}{i2} - \\text{MSE}{i3}$\n  Repeat the step 1 to 4 for all window length, find the length that maxmized the $\\text{MSE}$ as our preferred window length\n  We can also repeat the previous step for different non-parametric splines, and the one that maximized the $\\text{MSE}$ will be used to detect the outliers.\n5. Imputation We developed 7 different ways of imputation. Currently, we would prefer to impute the outliers with the fitted value of the newly fitted non-parametric splines using the data without outliers detected in the previous voting method.\n6. Result of the voting method knitr::include_graphics(\u0026quot;r1.png\u0026quot;) knitr::include_graphics(\u0026quot;r2.png\u0026quot;)  knitr::include_graphics(\u0026quot;r3.png\u0026quot;) knitr::include_graphics(\u0026quot;r4.png\u0026quot;)  knitr::include_graphics(\u0026quot;r5.png\u0026quot;) knitr::include_graphics(\u0026quot;r6.png\u0026quot;)  knitr::include_graphics(\u0026quot;r7.png\u0026quot;) knitr::include_graphics(\u0026quot;r8.png\u0026quot;)  The first and second columns are the original data; the second and fourth columns are the one after applying the voting methods and imputing outliers using the imputation method mentioned above.\nIn general, the voting methods removed the outliers and kept the trend we need in the functional data regression and other analyses using cumulated and aggregated measures of the values.\n7. Improvement in prediction 7.1 AUC in original and smoothed data set (processed by the voting method) We applied a logistic regression using survival as the outcome and mean oxygen level, APACHE score, CPR duration, and CPR derivatives as the predictor on the full data set and the processed data set, respectively.\nWe evaluate the AUC using a 5 fold cross-validation. We repeat the 5 fold cross-validation 1000 times to obtain the empirical distribution of the estimated AUC.\nknitr::include_graphics(\u0026quot;e1.png\u0026quot;) knitr::include_graphics(\u0026quot;e2.png\u0026quot;)  We were able to improve the mean of the AUC by 6 percent. However, outcome survival is a rare outcome, and AUC might not be able to give us a valid estimate.\n7.2 AUC in the bootstrapped original and smoothed data set (processed by the voting method) Since the outcome survival is a rare event, we applied bootstrap on the dataset first to enlarge the size of the data to five times as the original data and then applied the same analysis to estimate the AUC\nknitr::include_graphics(\u0026quot;e3.png\u0026quot;) knitr::include_graphics(\u0026quot;e4.png\u0026quot;)  The bootstrapped sample also shows an increase in the accuracy after removing the outliers. The AUC was increased by 6% after removing outliers using our voting method.\nHowever, since it is a bootstrapped sample, the data used for fitting the model may appear in the validation dataset, the AUC is overestimated. We would like to purpose some other estimation of the prediction accuracy in the future to perform the analysis again.\n8. Future works 8.1 Simulation To be finished, we purpose to simulate the functional data and the outlier generating process. We will apply the voting method on the simulated dataset to see if the method works well.\n8.2 R function and packages I would like to wrap up the method and R function, and test the method on more simulation data and generalize this method in terms of more types of outlier detections.\nI would also like to see if there is any other way to improve the method on its computational efficiency.\n","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"06aca1d08740a0f4297d766ea18161e4","permalink":"https://hanchao-zhang.github.io/post/fda/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/post/fda/","section":"post","summary":"1. Introduction The effects of outliers in the functional data analysis are well aware by statisticians in recent years, there have been some works of literature on finding outliers in the functional data.","tags":null,"title":"A Machine Learning Algothrim for Removing the Outliers in Functional Data","type":"post"},{"authors":null,"categories":null,"content":"1. Introduction In this project, we reviewed papers about time-dependent cox model, Therneau, T., Crowson, C., \u0026amp; Atkinson, E. (2013). Using time-dependent covariates and time dependent coefficients in the Cox model. Red, 2, 1., joint modeling with survival and longitudinal process, Hsieh, Fushing, Yi-Kuan Tseng, and Jane-Ling Wang. 2006. \u0026ldquo;Joint Modeling of Survival and Longitudinal Data: Likelihood Approach Revisited.\u0026rdquo; Biometrics 62 (4). Blackwell Publishing Inc: 1037-43.doi:10.1111/j.1541-0420.2006.00570.x., landmarking analysis, Dafni, U. (2011). Landmark analysis at the 25-year landmark point. Circulation: Cardiovascular Quality and Outcomes, 4(3), 363-371.\nApart from the model\u0026rsquo;s section, we also reviewed several papers to evaluate the prediction accuracy for the models above. Rizopoulos, D., Molenberghs, G., \u0026amp; Lesaffre, E. M. (2017). Dynamic predictions with time-dependent covariates in survival analysis using joint modeling and landmarking. Biometrical Journal, 59(6), 1261-1276. is the main one that we used to perform the simulation and real data analysis.\n2. Modelings and Methods In this section, we will introduce the three models that we reviewed, we also focused on the differences between the three models and the advantages and disadvantages of the three models.\n2.1 Time dependent covariates Cox models Before introducing the time-dependent covariates cox model, we want to refresh our memory on the proportional hazard model (cox model), which we define the hazard function as\n$$\\lambda(t|Z) = \\lambda_0(t)\\exp{\\beta\u0026rsquo; Z} $$\nwhere the baseline hazard $\\lambda_0(t)$ depends on time $t$ but not depends on the covariates $Z$ and the hazard ratio $e^{\\beta\u0026rsquo; Z}$ depends on covariates $Z$ but not depends on time $t$.\nThere are cases where if we measure some of the $Z_j$ over time, they may vary ( e.g. biomarkers)\nOften time, a small fraction of the available biomarker information is used. One example that motivated the time depends covariates cox model is the Stanford Heart transplant example, where a standard cox model was initially fit without consideration on the time-dependent covariate waiting time to get transplants which can be an important both in inference and prediction, the patients get the transplant earlier might have a higher survival probability due to the better condition of the early-stage patients.\nWhen we have such time-dependent covariates, we might want to modify the cox model so that it can take account for the time-dependent effect on the measured biomarkers. We can rewrite the hazard function where the exponential part was expressed by a linear combination of the covariates that depends on time:\n$$\\lambda(t) = \\lambda_0(t)\\exp{\\beta\u0026rsquo; Z(t)}$$\nit looks like a tiny modification, but it has a different meaning when looking at what do these two models measure.\nfor the cox model, we are comparing the survival distributions between those patients with different treatment. However, for the time-dependent covariates cox model, we are actually comparing the risk of an event between transplant and non-transplant at each event time, but we re-evaluated each person based on whether they had had a transplant by that time.\nInference proceeds similarly to the Cox model (time-independent), the only difference is that the values of Z now changes at each risk set. We can calculate the partial likelihood as follow:\n$$\\mathcal{L}(\\beta) = \\prod_{i=1}^n \\frac{\\exp{\\beta Z_i(T_i)}}{\\sum_{j \\in \\mathcal{R}(t_i)(\\beta\u0026rsquo; Z_j(T_i) }}\\delta_i$$\nHowever, one important feature for the time dependents covariates model is that the external covariates are more appropriate for the model compared to the internal covariates. To be specific, we want the covariates depends on time but not depends on the failure to be included in the time-dependent covariates cox model instead of the covariates that we cannot observe or dependents on the failure of the patients.\n2.2 Joint Models with longitudinal process and survival process To account for the special features of internal covariates, joint models for longitudinal and time-to-event data have been developed.\nIn the joint modeling, we need two outcome variables, a longitudinal outcome, and a survival outcome, instead of only one survival outcome in the previous time-dependent covariates cox model.\nThe joint model was combined by the process of a longitudinal mixed effect submodel, a survival submodel and a joint model of the above two submodels.\nFor the longitudinal process, we have a mixed effect model: $$y_i(t) = m_i(t) + \\epsilon_i(t) = \\boldsymbol{x_i}^T\\boldsymbol{\\beta} + \\boldsymbol{Z_i}(t)\\boldsymbol{b_i} + \\epsilon_i $$ where $y_i(t)$ denotes the observed longitudianl outcome at time $t$. $\\boldsymbol{x_i}$, $\\boldsymbol{z_i}$ denote the fix effect and random effect at time $t$ respectively, and the $m_i(t)$ denotes the predicted longitudinal outcome using the fix effect and random effect at time $t$.\nFor the survival process, we have a cox model: $$h_i(t|\\mathcal{M}_i(t)) = h_0(t)\\exp{\\gamma^T w_i + \\alpha m_i(t) } $$\nwhere $\\mathcal{M}_i(t) = {m_i(s), 0 \u0026lt; s \u0026lt; t }:$ the history of the true (unobserved) longitudinal process up to time $t$, $\\alpha$ quantifies the association between the true value of the marker at time $t$ and the hazard for an event and $w_i$ is a vector of baseline covariates.\nWe are able to rewrite the survival probability as follow: $$S_j{t |\\mathcal{M}_i(t, b_j), \\theta} = \\exp{\\int_0^t h_0(s)\\exp{\\gamma^T w_j + \\alpha_j(s) } } $$\nThe joint likelihood function is: $$p(y_j, T_i, \\delta_i) = \\int p(y_i|b_i){h(T_i|b_i)^{\\delta_i}S(T_i|b_i)}p(b_i)db_i $$ The likelihood of the model is derived under the conditional independence assumptions that given the random effects, both longitudinal and event time process are assumed independent, and the longitudinal response of each subject are assumed independent.\nEstimation of the joint models\u0026rsquo; parameters can be based either on maximum likelihood or a Bayesian approach using Markov chain Monte Carlo (MCMC) algorithms.\nWe can also use the joint model to do dynamic prediction, using the covariates recorded up to time $t$, we can predict the survival probability $\\pi(u|t)$ at time $u$ where $u\u0026gt;t$ $$\\pi(u|t) = \\Pr(T_j^* \\ge u | T_j^* \u0026gt; t, y_j(t), D_n)$$ Where $T_j^*$ is the true event time for subject $j$, and $D_n$ is the set ${T_i, \\delta_i, y_i }$. Meanwhile, when we have new information at time $t\u0026rsquo; \u0026gt; t$, we can update to obtain $\\pi_j(u|t\u0026rsquo;)$\n2.3 Landmarking Landmarking can also provide the survival probability $\\pi_j(u|t)$ for patient $j$ with resetting the time to zero being the landmarking time.\n$$h_i(u) = \\lim_{\\Delta t \\rightarrow 0}\\frac{1}{\\Delta t}\\Pr(u \\le T_i^*\u0026lt;u + \\Delta t | T_i^* \\ge u$$\n$$\\mathcal{Y}_i(t) = h_0(u)\\exp{\\gamma^T w_i + \\alpha \\tilde{y}_i(t) }$$\nwhere $w_i$ denotes the baseline covariates and $\\tilde{y}_i(t)$ denotes the last available longitudinal response that also enters into the model as an ordinary baseline covariate. The survival probability $\\pi_j(u|t)$ at time $t$ can be obtained by the mean of the Breslow estimator: $$\\hat\\pi^{LM}_j(u|t) = \\exp{-\\hat H_0(u)exp{\\hat \\gamma^T w_l + \\hat \\alpha \\tilde y_j(t) } }$$\nwhere\n$$\\hat H_0(u) = \\sum_{i\\in \\mathcal{R}(t)} \\frac{I(T_i \\le u) \\delta_i}{\\sum_{l \\in \\mathcal{R}(u)} \\exp{\\hat \\gamma^T w_l + \\hat \\alpha \\tilde y_l(t) } }$$ However, comparing to the joint modeling, this approach have several disadvantages:\n  A potential responder will only belong to the \u0026lsquo;responder\u0026rsquo; group if they survives until time of response. (Immortal time bias)\n  Responder v.s. non-responder is something that is not known at baseline, which makes the grouping based on something that will happen in the future that leads to the unpromising facts that any subjects who were lost to follow-up or died prior to landmark time are excluded from further analysis.\n  An important feature in landmarking is landmarking time. The result of survival analysis can be varied with different landmark times since the event happened before the landmark time is excluded from the analysis. We will obtain a smaller sample size and statistical power with later landmark time. However, we are prone to miss-classify the late responder to non-responder if we have an earlier landmark time. So, the choice of landmark time is arbitrary and crucial.\n3. Measuring model predictive performance After we reviewed those modeling methods, an important factor in evaluating the models is the predictive performance of the models. Here, we first define two evaluation methods, Discrimination, and Calibration. Then, we evaluated the three models using the two methods on real data (not the original data but a data set simulated based on the original real data) and simulated data.\n3.1 Discrimination Instead of calculating the discrimination of a time point, we focus on a time interval where we can take into account the dynamic nature of the longitudinally measured biomarker. The time interval also has medical relevance during which the physicians can take action to improve the survival probability of the patients who are about to go through events. They defined their prediction rules as follow:\n  We term subject $j$ as a case if the survival probability of patient $j$ is smaller than a threshold $c$, $\\pi_j(t + \\Delta t | t) \u0026lt; c$\n  Conversely term subject $j$ as a control if the survival probability of patient $j$ is larger than a threshold $c$, $\\pi_j(t + \\Delta t | t) \u0026gt; c$ for any $c \\in [0,1]$\n  For any randomly chosen pair of subjects ${i, j}$, the model can be assessed by the area under the curve using different $c$\n  If subject $j$ experiences the event within the time interval, they should be assigned a lower survival probability by the model\n  We can estimate the AUC by appropriately counting the concordant pairs of subjects. Meanwhile, we decompose AUC into 4 parts first and then weighted each AUC by the probability of they will be comparable, meaning that we weight the AUC by the probability of the senior happening.\nand thus, we have the estimation of $AUC$ expressed as follow: $$\\hat{AUC}m = \\frac{\\sum{i = 1}^n \\sum_{j = 1, j\\ne i}^n I{\\hat\\pi_i(t + \\Delta t|t) \u0026lt; \\hat\\pi_j(t + \\Delta t|t) \\times I{\\Omega_{ij}^{(m)}(t) \\times \\hat \\nu_{ij}^{(m)} } } }{\\sum_{i = 1}^n \\sum_{j = 1, j\\ne i}^n I{\\Omega_{ij}^{(m)} (t) \\times \\nu_{ij}^{(m)} } }$$\nwhere $m = 2,3,4$ and $\\hat \\nu_{ij}^2 = 1 - \\hat\\pi_i(t + \\Delta t | T_i)$, $\\hat \\nu_{ij}^3 = \\hat\\pi_i(t + \\Delta t | T_i)$, $\\hat \\nu_{ij}^4 = 1 - \\hat\\pi_i(t + \\Delta t | T_i)\\times\\hat\\pi_i(t + \\Delta t | T_i) $\n3.2 Calibration Similar to the discrinimation, we also want to take account into the longitudinal nature of the biomaker. And the goal is to predict the occurrence of events at $u \u0026gt; t$ given the information recorded up to time $t$. $$PE(u | t) = E[L {I(T^* \u0026gt; u) - \\pi(u|t) } ] $$ An estimate of $PE(u|t)$ considering censoring was proposed by Henderson et al.(2002):\n$\\hat PE(u|t)= { n(t)}^{-1} \\sum_{i: T\u0026gt;t} I(T_i \u0026gt; u)L{ 1 - \\hat\\pi_i(u|t) } + \\delta_i I(T_i \u0026lt; u)L{ 0 - \\hat\\pi_i(u|t) } $\n$$+ (1 - \\delta_i)[ \\hat\\pi_i(u|T_i)L{ 1 - \\hat\\pi_i(u|t) } + (1 - \\hat\\pi_i(u|T_i))L{ 0 - \\hat\\pi_i(u|t) }]$$\nwhere ${ n(t)}^{-1}$ is the number of subjects at riskt at time $t$, $I(T_i \u0026gt; u)L{ 1 - \\hat\\pi_i(u|t) }$ denotes the patients who were alive after time $u$, $\\delta_i I(T_i \u0026lt; u)L{ 0 - \\hat\\pi_i(u|t) }$ denotes the patients who were died before time $u$, and $ (1 - \\delta_i)[ \\hat\\pi_i(u|T_i)L{ 1 - \\hat\\pi_i(u|t) } + (1 - \\hat\\pi_i(u|T_i))L{ 0 - \\hat\\pi_i(u|t) }]$ denotes the patients who are censored at time $u$.\n4. Aortic valve dataset analysis} Some facts of the dataset:\n 285 patients who received a human tissue valve in the aortic position in hospital from 1987 to 2008 77(27%) of the patients received a subcoronary implantation(SI) and 208 of the patients received root replacement(RR) 73 of the patients required reoperation and 59 patients had died after operation Echo examinations were scheduled at 6 months and 1 year postoperatively, and biennially thereafter. The aortic gradient was taken at each examination  we can find that there are minimal differences in the reoperation-free survival rates between RR and SI, and considerable variability in the shape of these trajectories, meanwhile no systematic differences apparent between the two groups. The following table is from rizopoulos2017dynamic paper, showing the prediction accuracy and AUC for three models.\nWhere the three models are, $$y_i(t) = \\beta_1 SI_i + \\beta_2 RR_i + \\sum_{k=1}^3 \\beta_{2k+1} { \\beta_k(t, \\lambda) \\times RR_i } + b_{i0} + \\sum_{k=1}^3b_{ik}\\beta_k(t, \\lambda) + \\epsilon_i(t) $$ for the longitudinal process, and $M_1$, $M_2$, $M_3$ for the survival process:\n$M_1: h_i(t) = h_0(t)\\exp{ \\gamma_1 RR_i + \\gamma_2 Age_i + \\gamma_3 Female_i + \\alpha_1 m_i(t)}$\n$M_2: h_i(t) = h_0(t)\\exp{ \\gamma_1RR_i + \\gamma_2Age_i + \\gamma_3Female_i + \\alpha_1m_i(t)}$\n$M_3: h_i(t) = h_0(t)\\exp{ \\gamma_1RR_i + \\gamma_2Age_i + \\gamma_3Female_i + \\alpha_1\\int_0^tm_i(s)ds}$\n5. Simulation Data Analysis Some facts of the data:\n 1000 patients have been followed up for 15 years Provide Longitudinal measurements at baseline and afterward at 14 random follow up times Simulated the longitudinal process using B-splines Simulated the survival process using Weibull model Simulate the censoring time dependents on treatment  they used the same linear mixed effect with B-spline model to simulate the longitudinal process: $$y_i(t) = \\beta_1Trt0_i + \\beta_2Trt1_i + beta_3(t\\times Trt0_i) + \\beta_3(t \\times Trt1_i) + \\epsilon_i(t)$$ and used survival model as follow to simuate the survival porcess:\n$\\textbf{Scenario I:} \\hspace{2mm} h_i(t) = h_0(t)\\exp {\\gamma_1Trt1_i + \\alpha_1m_i(t) }$a $\\textbf{Scenario II:} \\hspace{2mm} h_i(t) = h_0(t)\\exp {\\gamma_1Trt1_i + \\alpha_1m_i(t) + \\alpha_2m\u0026rsquo;_i(t)}$ $\\textbf{Scenario III:} \\hspace{2mm} h_i(t) = h_0(t)\\exp {\\gamma_1Trt1_i + \\alpha_1\\int_0^tm_i(s)ds }$\n6. results Joint modeling approaches outperform the landmark approaches when there is no misspecification on the time effect on the longitudinal process. Landmark approaches outperformed the joint models in a few cases when there is strong misspecification of the time effect. Misspecification of the functional form that links the two processes did not seem to particularly influence the performance of the joint modeling and landmark approaches.\\cite{rizopoulos2017dynamic}\\cite{rizopoulos2014r}\\cite{rizopoulos2014combining}\n","date":1576800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576800000,"objectID":"4c7f0860685cfa21dcb03cdc6176c1fa","permalink":"https://hanchao-zhang.github.io/post/jointmodeling/","publishdate":"2019-12-20T00:00:00Z","relpermalink":"/post/jointmodeling/","section":"post","summary":"1. Introduction In this project, we reviewed papers about time-dependent cox model, Therneau, T., Crowson, C., \u0026amp; Atkinson, E. (2013). Using time-dependent covariates and time dependent coefficients in the Cox model.","tags":null,"title":"Paper Review Time-dependent ROCs and AUCs for censored endpoints, dynamic prediction via joint modeling and landmarking","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://hanchao-zhang.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d42ce4653e629ffdcbf59ee86ee5e022","permalink":"https://hanchao-zhang.github.io/etcetera/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/etcetera/","section":"","summary":"Others","tags":null,"title":"Etcetera Page","type":"widget_page"},{"authors":null,"categories":null,"content":"Overview The Open Case Studies project is an educational resource that educators can use in the classroom to teach students how to effectively derive knowledge from data in real-world challenges.\nLink to our website https://opencasestudies.github.io/\n","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"7f0d630b9f2d29522b21a496725ba073","permalink":"https://hanchao-zhang.github.io/post/ocs/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/post/ocs/","section":"post","summary":"Overview The Open Case Studies project is an educational resource that educators can use in the classroom to teach students how to effectively derive knowledge from data in real-world challenges.","tags":null,"title":"Open Case Study Project Working Group","type":"post"},{"authors":null,"categories":null,"content":"1. Question 1 Jaccard similarity The definition of the Jaccard Similarity matrix is written as follow $$Jac(A,B)=\\frac{|A\\bigcap B|}{|A\\bigcup B|}$$\nHowever, the way of calculating Jaccard Matrix is little different. THe way of computing the matrix is showed as follow\n$$Jac(A,B)=\\frac{|A = B = 1|}{|A = 1 ~ or ~ B = 1|}$$\nThe two figures show the variance explained and the cumulative variance explained by PCA. About 200 PCA can explain 80% of the variance.\nHowever, as the plot below shows, the first two PCA can separate the two classes, Han Chinese in Beijing and Japanese in Tokyo pretty good.\n2. Question 2 Classification Model for Cardiac Arrhythmia Methods can be applied:\n LDA QDA Best Subset Forward Selection Backward Selection Stepwise Selection Ridge Regression Lasso regression Elastic Net Decision Tree Bagging of the Trees Random Forest Gradient Boosting ADA boosting K-means Hierarchical clustering  Methods not applied:\n  LDA is not used since it requires each covariate to be normally distributed.\n  Unsupervised learning is also not used since the label of the data is known.\n  Splines are not used also since most of the predictors are binary variables.\n  2.1 General Linear Model and KNN with variables selected by correlation The x lab of the figure is the number of the variables included in the model (selected by correlation).\nAs the figure shows, the Logistic regression has lower cross-validation error.\n2.2 General Linear Model with variables selected by AIC     Stepwise Forward Backward subset     error 0.20 0.33 0.28 0.25    2.3 Tree Based Methods The data was imputed at first, the column with missing value more than 80% was then deleted.\nAs the figure shows, the Random Forest model has the lowest cv error.\n2.4 Shrinkage Methods The Elastic net returns to the minimum cv error.\n3. Model Comparison     Methods(parameters) cv-error     Logistic(Number of covariates = 38) 0.14   KNN (Number of covariates = 21) 0.33   Simple Tree(ntree = 23) 0.17   Random Forest (ntree = 122, mtry = 49) 0.12   Vagging of the Tree (ntree = 232, mtry = 279) 0.14   Gradient Boosting(ntree = 123, depth = 1) 0.15   ADA Boosting(ntree = 242, depth = 1) 0.15   Stepwise Selection 0.20   Forward Selection 0.33   Backward Selection 0.28   Best Subset Selection 0.25   Elastic Net ($\\alpha = 0.7$, $\\lambda = 29$) 0.17   Lasso ($\\lambda = 22$) 0.18   Ridge ($\\lambda = 26$) 0.18     The random Forest returns the best model with the lowest prediction error.\nCompare to the paper. The prediction error is much smaller than the VFI5 method. However, the VFI5 method can categorize the result into 16 classes, it makes sense that VFI5 model has higher prediction error.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715  ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) library(MASS) library(tidyverse) library(nnet) library(foreach) library(doMC) library(class) library(glmnet) library(caret) library(tree) library(randomForest) library(pROC) library(gbm) library(xgboost) library(xtable) library(glmnet) registerDoMC(10) ``` # Q1 ```{r} dat.hapmap \u0026lt;- read.csv(\u0026#34;/Users/hanchao/Library/Mobile Documents/com~apple~CloudDocs/ Courses/Cornell Semester 3/Statistical Learning/FinalProject/SimulatedHapMap.csv\u0026#34;) ``` ```{r} dat.exm \u0026lt;- dat.hapmap[,-1] for (i in 1:ncol(dat.exm)) { if(!is.numeric(dat.exm[,i])){ dat.exm[,i] \u0026lt;- as.numeric(as.character(dat.exm[,i])) } } dat.exm[dat.exm \u0026gt; 0] \u0026lt;- 1 dat.exm \u0026lt;- as.data.frame(t(dat.exm)) t1 \u0026lt;- Sys.time() smi \u0026lt;- list() for (i in 1:ncol(dat.exm)) { a1 = dat.exm[,i] == dat.exm[,1:ncol(dat.exm)] \u0026amp; dat.exm[,i] != 0 a2 = dat.exm[,i] != 0 | dat.exm[,1:ncol(dat.exm)] != 0 smi[[i]] \u0026lt;- colSums(a1)/colSums(a2) print(i) } dat.jaccard \u0026lt;- do.call(cbind, smi) dat.jaccard t2 \u0026lt;- Sys.time() t2-t1 dim(dat.jaccard) ``` Time difference of 9.851856 mins Modify Chunk OptionsRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current Chunk Console~/ Console Terminal ~/\t```{r} write.csv(dat.jaccard, \u0026#34;/Users/hanchao/Library/Mobile Documents/ com~apple~CloudDocs/Courses/Cornell Semester 3/Statistical Learning/FinalProject/jaccard.csv\u0026#34;) ``` ```{r} dat.jaccard \u0026lt;- read.csv(\u0026#34;/Users/hanchao/Library/Mobile Documents/ com~apple~CloudDocs/Courses/Cornell Semester 3/Statistical Learning/FinalProject/jaccard.csv\u0026#34;) dat.jaccard \u0026lt;- dat.jaccard[,2:ncol(dat.jaccard)] ``` ```{r} obj.prcomp \u0026lt;- prcomp(dat.jaccard, scale = TRUE) ``` ```{r} par(mfrow = c(1,2)) plot( (obj.prcomp$sdev^2)/sum(obj.prcomp$sdev^2), type = \u0026#34;l\u0026#34;, ylab = \u0026#34;variance explained\u0026#34;) points(x = 2, y = ((obj.prcomp$sdev^2)/sum(obj.prcomp$sdev^2))[2], col = \u0026#34;red\u0026#34;, pch = 20) plot( cumsum(obj.prcomp$sdev^2 )/ sum(obj.prcomp$sdev^2) , type = \u0026#34;l\u0026#34;, ylab = \u0026#34;percentage of variance explained\u0026#34;) points(x = 2, y = ((cumsum(obj.prcomp$sdev^2 )/ sum(obj.prcomp$sdev^2)))[2], col = \u0026#34;red\u0026#34;, pch = 20) ``` ```{r} plot(-obj.prcomp$x[,1],obj.prcomp$x[,2], col = dat.hapmap$X, pch = 20, xlab = \u0026#34;First PCA\u0026#34;, ylab = \u0026#34;Second PCA\u0026#34;) legend(\u0026#34;top\u0026#34;, y = as.character(unique(dat.hapmap$X)), col = c(\u0026#34;black\u0026#34;, \u0026#34;red\u0026#34;), pch = 20) ``` ```{r} dat.ggplot \u0026lt;- data.frame(pca1 = obj.prcomp$x[,1], pca2 = obj.prcomp$x[,2], label = dat.hapmap$X) ``` # Q 2 ```{r} dat.arr \u0026lt;- read.table(\u0026#34;/Users/hanchao/Library/Mobile Documents/com~apple~CloudDocs/ Courses/Cornell Semester 3/Statistical Learning/FinalProject/arrhythmia.data\u0026#34;, sep = \u0026#34;,\u0026#34;, header = F) names(dat.arr)[ncol(dat.arr)] \u0026lt;- \u0026#34;Y\u0026#34; dat.arr$Y \u0026lt;- ifelse(dat.arr$Y \u0026gt; 1, yes = 1, no = 0) ``` ```{r} for (i in names(dat.arr)) { if(!is.numeric(dat.arr[,i])){ print(i) } } for (i in names(dat.arr)) { dat.arr[,i][dat.arr[,i] == \u0026#34;?\u0026#34;] \u0026lt;- NA dat.arr[,i] \u0026lt;- as.numeric(dat.arr[,i]) if(max(dat.arr[,i], na.rm = T) == 16 \u0026amp; min(dat.arr[,i], na.rm = T) == 1){ print(i) } } for (i in names(dat.arr)) { if( mean(is.na(dat.arr[,i])) \u0026gt; 0.5 ){ print(i) } } #dat.arr \u0026lt;- dat.arr[,-which(names(dat.arr) == \u0026#34;V14\u0026#34;)] ``` ## LDA ```{r} cv.lda \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] data.frame( vairable = 1:279, cor = as.numeric(cor(dat.arr$Y, dat.arr[,-280])) ) %\u0026gt;% arrange(desc(abs(cor))) -\u0026gt; dat.cor } k.error \u0026lt;- NULL for (K in k) { fit.lda \u0026lt;- lda(Y~., data = train[, c(dat.cor$vairable[1:K], which(names(train) == y)) ] ) pred \u0026lt;- predict(fit.lda, newdata = valid) k.error[K] \u0026lt;- mean(pred$class != valid$Y) } k.error } err.lda \u0026lt;- cv.lda(data = dat.arr, n = 5, k = 1:100, y = \u0026#34;Y\u0026#34;) ``` ```{r} cv.lda \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (K in k) { k.error \u0026lt;- NULL for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] data.frame( vairable = 1:279, cor = as.numeric(cor(dat.arr$Y, dat.arr[,-280])) ) %\u0026gt;% arrange(desc(abs(cor))) -\u0026gt; dat.cor dat.train \u0026lt;- train[,c(dat.cor$vairable[1:K],280)] dat.valid \u0026lt;- valid[,c(dat.cor$vairable[1:K],280)] fit.lda \u0026lt;- lda(Y~., data = dat.train) pred \u0026lt;- predict(fit.lda, newdata = dat.valid) k.error[i] \u0026lt;- mean(pred$class != dat.valid$Y) } knn.error[K] \u0026lt;- mean(k.error) } z \u0026lt;- data.frame(k = k, error = knn.error) z } err.lda \u0026lt;- cv.lda(data = dat.arr, n = 10, k = 1:50, y = \u0026#34;Y\u0026#34;) err.lda[which.min(err.lda$error),] ``` ## Logistic ```{r} cv.logistic \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] data.frame( vairable = 1:279, cor = as.numeric(cor(dat.arr$Y, dat.arr[,-280])) ) %\u0026gt;% arrange(desc(abs(cor))) -\u0026gt; dat.cor } k.error \u0026lt;- NULL for (K in k) { fit.lda \u0026lt;- glm(Y~., data = train[, c(dat.cor$vairable[1:K], which(names(train) == y)) ] , family = binomial() ) pred \u0026lt;- predict(fit.lda, newdata = valid, type = \u0026#34;response\u0026#34;) dat.roc \u0026lt;- roc(valid[,y]~pred) thres \u0026lt;- coords(dat.roc,\u0026#34;best\u0026#34;)[1] pred \u0026lt;- ifelse(pred \u0026lt; thres, yes = 0, no = 1) k.error[K] \u0026lt;- mean(pred != valid$Y) } k.error } err.logistic \u0026lt;- cv.logistic(data = dat.arr, n = 5, k = 1:100, y = \u0026#34;Y\u0026#34;) ``` ```{r} cv.multinomial \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (K in k) { k.error \u0026lt;- NULL for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] data.frame( vairable = 1:279, cor = as.numeric(cor(dat.arr$Y, dat.arr[,-280])) ) %\u0026gt;% arrange(desc(abs(cor))) -\u0026gt; dat.cor dat.train \u0026lt;- train[,c(dat.cor$vairable[1:K],280)] dat.valid \u0026lt;- valid[,c(dat.cor$vairable[1:K],280)] fit.lda \u0026lt;- glm(Y~., data = dat.train, family = binomial) #fit.lda \u0026lt;- stepAIC(fit.lda) pred \u0026lt;- predict(fit.lda, newdata = dat.valid, type = \u0026#34;response\u0026#34;) dat.roc \u0026lt;- roc(dat.valid$Y ~ pred) thres \u0026lt;-coords(dat.roc, \u0026#34;best\u0026#34;)[1] pred \u0026lt;- ifelse(pred \u0026lt; thres, yes = 0, no = 1) k.error[i] \u0026lt;- mean(pred != dat.valid$Y) } knn.error[K] \u0026lt;- mean(k.error) } z \u0026lt;- data.frame(k = k, error = knn.error) z } err.multinomial \u0026lt;- cv.multinomial(data = dat.arr, n = 5, k = 1:100, y = \u0026#34;Y\u0026#34;) err.multinomial[which.min(err.multinomial$error),] ``` ## KNN ```{r} cv.knn \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (K in k) { k.error \u0026lt;- NULL for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] Train \u0026lt;- data.frame(scale(train[,-which(names(train) == y)]), Class = train[,y]) Valid \u0026lt;- data.frame(scale(valid[,-which(names(valid) == y)]), Class = valid[,y]) pred.knn \u0026lt;- knn(Train[,-which(colnames(Train) == \u0026#34;Class\u0026#34;)], Valid[,-which(colnames(Valid) == \u0026#34;Class\u0026#34;)], Train[,\u0026#34;Class\u0026#34;], k = K) k.error[i] \u0026lt;- mean(pred.knn != Valid[,\u0026#34;Class\u0026#34;]) } knn.error[K] \u0026lt;- mean(k.error) } z \u0026lt;- data.frame(k = k, knn.error) z } dat.arr.knn \u0026lt;- dat.arr[,colMeans(is.na(dat.arr)) == 0] for (i in names(dat.arr.knn)) { if(mean(duplicated(dat.arr.knn[,i])) \u0026gt; 0.9 ){ dat.arr.knn[,i] \u0026lt;- NA } } dat.arr.knn \u0026lt;- dat.arr.knn[,colMeans(is.na(dat.arr.knn)) == 0 ] dat.arr.knn \u0026lt;- data.frame(dat.arr.knn, Y = dat.arr$Y) err.knn \u0026lt;- cv.knn(data = dat.arr.knn, n = 5, k = 1:100, y = \u0026#34;Y\u0026#34;) ``` ```{r} cv.qda \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] data.frame( vairable = 1:279, cor = as.numeric(cor(dat.arr$Y, dat.arr[,-280])) ) %\u0026gt;% arrange(desc(abs(cor))) -\u0026gt; dat.cor } k.error \u0026lt;- NULL for (K in k) { fit.lda \u0026lt;- qda(Y~., data = train[, c(dat.cor$vairable[1:K], which(names(train) == y)) ] ) pred \u0026lt;- predict(fit.lda, newdata = valid) k.error[K] \u0026lt;- mean(pred$class != valid$Y) } k.error } ``` ## Stepwise ```{r} cv.backward \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;, direction = \u0026#34;both\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] fit.lda \u0026lt;- glm(Y~., data = train, family = binomial() ) fit.lda \u0026lt;- step(fit.lda, glm(Y~., data = train, family = binomial()), direction = direction) pred \u0026lt;- predict(fit.lda, newdata = valid, type = \u0026#34;response\u0026#34;) dat.roc \u0026lt;- roc(valid[,y]~pred) thres \u0026lt;- coords(dat.roc,\u0026#34;best\u0026#34;)[1] pred \u0026lt;- ifelse(pred \u0026lt; thres, yes = 0, no = 1) k.error \u0026lt;- mean(pred != valid$Y) } k.error } err.forward \u0026lt;- cv.backward(data = dat.arr.knn, n = 5, k = 1:100, y = \u0026#34;Y\u0026#34;, direction = \u0026#34;forward\u0026#34;) err.stepwise \u0026lt;- cv.backward(data = dat.arr.knn, n = 5, k = 1:100, y = \u0026#34;Y\u0026#34;, direction = \u0026#34;both\u0026#34;) err.backward \u0026lt;- cv.backward(data = dat.arr.knn, n = 5, k = 1:100, y = \u0026#34;Y\u0026#34;) tbl.aic \u0026lt;- data.frame(error = c(err.stepwise, err.forward,na.omit(err.backward))) rownames(tbl.aic) \u0026lt;- c(\u0026#34;Stepwise\u0026#34;, \u0026#34;Forward\u0026#34;, \u0026#34;Backward\u0026#34;) t(tbl.aic) %\u0026gt;% xtable() ``` ```{r} plot(err.multinomial, type = \u0026#34;l\u0026#34;,cex = 0.5, col = \u0026#34;red\u0026#34;, ylim = c(0.2,0.5), xlab = \u0026#34;Number of Variables\u0026#34;) lines(err.knn, col = \u0026#34;blue\u0026#34;) legend(x=\u0026#34;topleft\u0026#34;, legend = c( \u0026#34;Logistic\u0026#34;, \u0026#34;KNN\u0026#34;), col = c( \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;), pch = 20) ``` ## Tree ```{r} cv.tree2 \u0026lt;- function(data = data, n = \u0026#34;fold\u0026#34;, k = \u0026#34;knn-numer\u0026#34;, y = \u0026#34;Class\u0026#34;){ knn.error \u0026lt;- NULL dd \u0026lt;- data[sample(nrow(data), nrow(data), replace = F),] dat.splited \u0026lt;- split(dd, rep(1:n, each=nrow(dd)/(n))) for (K in k) { k.error \u0026lt;- NULL for (i in 1:n) { train \u0026lt;- dat.splited[-i] train \u0026lt;- do.call(rbind, train) valid \u0026lt;- dat.splited[[i]] my.tree \u0026lt;- tree(factor(Y)~., data = train) cv.model.pruned \u0026lt;- prune.misclass(my.tree, best=K) pred \u0026lt;- predict(cv.model.pruned, newdata = valid)[,2] dat.roc \u0026lt;- roc(valid$Y ~ pred) thres \u0026lt;- coords(dat.roc, \u0026#34;best\u0026#34;)[1] pred.knn \u0026lt;- ifelse(pred \u0026lt; thres, yes = 0, no = 1) k.error[i] \u0026lt;- mean(pred.knn != valid[,\u0026#34;Y\u0026#34;]) } knn.error[K] \u0026lt;- mean(k.error) } z \u0026lt;- data.frame(k = k, knn.error) z } err.tree \u0026lt;- NULL for (i in 2:500) { err.tree[i-1] \u0026lt;- as.numeric(na.omit(cv.tree2(data = dat.arr.tree, n = 5, k = i, y = \u0026#34;Y\u0026#34;)$knn.error)) } err.tree plot(err.tree, type = \u0026#34;l\u0026#34;) points(which.min(err.tree), min(err.tree), pch = 20, col = \u0026#34;red\u0026#34;) ``` ```{r} dat.arr.tree \u0026lt;- dat.arr[,-which(names(dat.arr) == \u0026#34;V14\u0026#34;)] my.tree \u0026lt;- tree(factor(Y)~., data = dat.arr.tree) mytree.cv \u0026lt;- cv.tree(my.tree,FUN=prune.misclass,K=10) plot(x = mytree.cv$size, y = mytree.cv$dev, type = \u0026#34;b\u0026#34;, xlab = \u0026#34;Size\u0026#34;, ylab = \u0026#34;Deviance\u0026#34;) points(x = mytree.cv$size[which.min(mytree.cv$dev)], y = mytree.cv$dev[which.min(mytree.cv$dev)], col = \u0026#34;red\u0026#34;) best.size \u0026lt;- mytree.cv$size[which(mytree.cv$dev==min(mytree.cv$dev))] cv.model.pruned \u0026lt;- prune.misclass(my.tree, best=best.size[1]) plot(cv.model.pruned) text(cv.model.pruned, use.n=TRUE, all=TRUE, cex=.8) ``` ## RF \u0026amp; bag of trees ```{r} hd.imputed \u0026lt;- rfImpute(Y~.-Y,data=dat.arr.tree) err.rf1 \u0026lt;- list() for (i in seq(1, 278, by = 1)) { obj.tree1 \u0026lt;- randomForest(factor(Y)~., data = hd.imputed, mtry=i,ntree=500) err.rf1[[i]] \u0026lt;- (obj.tree1$err.rate[,1]) } a \u0026lt;- do.call(cbind, err.rf1) err.rf \u0026lt;- apply(a, 2,min) plot(na.omit(err.rf), type = \u0026#34;l\u0026#34;, ylab = \u0026#34;Error\u0026#34;, xlab = \u0026#34;Split\u0026#34;, main = \u0026#34;Random Forest and Bagging of Tree\u0026#34;) points(which.min(err.rf),min(err.rf), pch = 20, col = \u0026#34;red\u0026#34;) ``` ## GBM ```{r} library(Matrix) ##need to exclude missing train.new \u0026lt;- na.omit(dat.arr.tree) test.new \u0026lt;- na.omit(valid) covariates_matrix = sparse.model.matrix(Y ~ ., data = train.new)[,-1] output_vector = train.new[, \u0026#39;Y\u0026#39;] == 1 #covariates_test_matrix = sparse.model.matrix(AHD ~ ., data = test.new)[,-1] #output_test_vector = test.new[, \u0026#39;AHD\u0026#39;] == \u0026#34;Yes\u0026#34; ##Create grid of parameters to try xgb_grid = expand.grid( eta = c(0.1, 0.01, 0.001, 0.0001), max_depth = c(1,2) ) for(i in 1:nrow(xgb_grid)){ assign(paste0(\u0026#34;xgb_params.\u0026#34;, i), list( objective = \u0026#34;binary:logistic\u0026#34;, eta = xgb_grid[i,1], max.depth = xgb_grid[i,2], eval_metric = \u0026#34;error\u0026#34; ))} param.list \u0026lt;- list(xgb_params.1, xgb_params.2, xgb_params.3, xgb_params.4, xgb_params.5, xgb_params.6,xgb_params.7,xgb_params.8) set.seed(12345) best.cv.error \u0026lt;- data.frame() Eval.cv \u0026lt;- list() for(i in 1:nrow(xgb_grid)){ xgb_cv = xgb.cv(params = param.list[[i]], data = covariates_matrix, label = output_vector, nrounds = 500, nfold = 5, prediction = TRUE, showsd = TRUE, stratified = TRUE, verbose = TRUE ) Eval.cv[[i]] \u0026lt;- xgb_cv$evaluation_log[,c(1,4)] best.cv.error[i,1] \u0026lt;- min(Eval.cv[[i]]$test_error_mean) } rownames(best.cv.error) \u0026lt;- c(\u0026#34;eta=0.1, depth=1\u0026#34;, \u0026#34;eta=0.01, depth=1\u0026#34;, \u0026#34;eta=0.001, depth=1\u0026#34;, \u0026#34;eta=0.0001, depth=1\u0026#34;, \u0026#34;eta=0.1, depth=2\u0026#34;, \u0026#34;eta=0.01, depth=2\u0026#34;, \u0026#34;eta=0.001, depth=2\u0026#34;, \u0026#34;eta=0.0001, depth=2\u0026#34;) which.min(best.cv.error$V1) ``` ```{r} library(\u0026#39;e1071\u0026#39;) ##find shrinakge parameter caretGrid.depth.1 \u0026lt;- expand.grid(interaction.depth=1, n.trees = 1000, shrinkage=seq(0, 0.01, by = 0.001), n.minobsinnode=10) set.seed(123) gbm.caret.depth.1 \u0026lt;- caret::train(factor(Y) ~ ., distribution = \u0026#34;adaboost\u0026#34;, data = train.new, method = \u0026#34;gbm\u0026#34;, trControl=trainControl(method=\u0026#34;cv\u0026#34;, number=5), verbose=F, tuneGrid=caretGrid.depth.1) best.tune.1 \u0026lt;- gbm.caret.depth.1$bestTune$shrinkage names(gbm.caret.depth.1) gbm.caret.depth.1$results caretGrid.depth.2 \u0026lt;- expand.grid(interaction.depth=2, n.trees = 1:250, shrinkage=seq(0, 0.01, by = 0.001), n.minobsinnode=10) set.seed(123) gbm.caret.depth.2 \u0026lt;- caret::train(factor(Y) ~ ., distribution = \u0026#34;adaboost\u0026#34;, data = train.new, method = \u0026#34;gbm\u0026#34;, trControl=trainControl(method=\u0026#34;cv\u0026#34;, number=5), verbose=F, tuneGrid=caretGrid.depth.2) gbm.caret.depth.2$results[,c(1,2,4,5)] best.tune.2 \u0026lt;- gbm.caret.depth.2$bestTune$shrinkage ##Fit the models with the parameters selected from cross validation ada.train \u0026lt;- train.new ada.train$AHD \u0026lt;- ifelse(ada.train$AHD==\u0026#34;No\u0026#34;, 0, 1) adaboost.depth.1 \u0026lt;- gbm(AHD ~ ., distribution=\u0026#34;adaboost\u0026#34;, data=ada.train, shrinkage=best.tune.1, n.trees=2000) adaboost.depth.2 \u0026lt;- gbm(AHD ~ ., distribution=\u0026#34;adaboost\u0026#34;, data=ada.train, shrinkage=best.tune.2, n.trees=2000) ##Predict with 500 trees pred.out.ada.depth.1 \u0026lt;- ifelse(predict(adaboost.depth.1, test.new, n.trees = 500, type=\u0026#34;response\u0026#34;) \u0026gt;0.5, \u0026#34;Yes\u0026#34;, \u0026#34;No\u0026#34;) misclass.ada.depth.1 \u0026lt;- mean(pred.out.ada.depth.1 != test.new$AHD) misclass.ada.depth.1 ``` ```{r} err.ada \u0026lt;- NULL for (i in 1:250) { caretGrid.depth.2 \u0026lt;- expand.grid(interaction.depth=2, n.trees = i, shrinkage=seq(0, 0.01, by = 0.001), n.minobsinnode=10) gbm.caret.depth.2 \u0026lt;- caret::train(factor(Y) ~ ., distribution = \u0026#34;adaboost\u0026#34;, data = train.new, method = \u0026#34;gbm\u0026#34;, trControl=trainControl(method=\u0026#34;cv\u0026#34;, number=5), verbose=F, tuneGrid=caretGrid.depth.2) err.ada[i] \u0026lt;- gbm.caret.depth.2$results$Accuracy[which.max(gbm.caret.depth.2$results$Accuracy)] } ``` ```{r} plot(na.omit(err.rf), type = \u0026#34;l\u0026#34;, ylab = \u0026#34;Error\u0026#34;, xlab = \u0026#34;Trees\u0026#34;, col = \u0026#34;blue\u0026#34;, ylim = c(0.12,0.45), xlim = c(1,240)) points(which.min(err.rf),min(err.rf), pch = 20, col = \u0026#34;red\u0026#34;) lines(err.tree, type = \u0026#34;l\u0026#34;, col = \u0026#34;dark blue\u0026#34;) points(which.min(err.tree), min(err.tree), pch = 20, col = \u0026#34;red\u0026#34;) lines(Eval.cv[[1]], col = \u0026#34;green\u0026#34;) points(which.min(Eval.cv[[1]]$test_error_mean), min(Eval.cv[[1]]$test_error_mean), pch = 20, col = \u0026#34;red\u0026#34;) lines((1-err.ada)) points(x = which.min(1-err.ada), y = min((1-err.ada)), pch = 20, col = \u0026#34;red\u0026#34;) legend(x = \u0026#34;topright\u0026#34;,legend = c(\u0026#34;Ada Boosting\u0026#34;, \u0026#34;Gradience Boosting\u0026#34;,\u0026#34;Randome Forest\u0026#34;, \u0026#34;Simple Tree\u0026#34;), col = c(\u0026#34;black\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;dark blue\u0026#34;), pch = 20) ``` ```{r} dat.arr.lasso \u0026lt;- na.omit(dat.arr.tree) x \u0026lt;- model.matrix(Y~.,dat.arr.lasso)[,-1] # First column corresponds to the intercept and is removed y \u0026lt;- dat.arr.lasso$Y l \u0026lt;- seq(0,1,by = 0.1) err.shrinkage \u0026lt;- list() for (i in 1:10) { enet.cv = cv.glmnet(x,y,alpha=l[i],nfolds=5) err.shrinkage[[i]] \u0026lt;- (enet.cv$cvm) } dat.lasso \u0026lt;- do.call(cbind,err.shrinkage) dat.lasso[30,] == min(err.lasso) err.lasso \u0026lt;- apply(dat.lasso, 1, min) which(dat.lasso == min(err.lasso)) plot(err.lasso, type = \u0026#34;l\u0026#34;, ylab = \u0026#34;Error\u0026#34;, xlab = \u0026#34;Lambda\u0026#34;) points(x = 1, err.lasso[1], col = \u0026#34;dark green\u0026#34;, pch = 20) points(x = length(err.lasso), err.lasso[length(err.lasso)], col = \u0026#34;blue\u0026#34;, pch = 20) points(x = which.min(err.lasso), min(err.lasso), col = \u0026#34;red\u0026#34;, pch = 20) legend(x = \u0026#34;top\u0026#34;,legend = c(\u0026#34;Ridge\u0026#34;, \u0026#34;Lasso\u0026#34;,\u0026#34;The Best Elastic\u0026#34;), col = c(\u0026#34;dark green\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;), pch = 20) ``` ```{r} tbl.cp \u0026lt;- data.frame(error = c(min(err.logistic), min(err.rf), min(err.stepwise), min(err.lasso) )) rownames(tbl.cp) \u0026lt;- c(\u0026#34;logistic\u0026#34;, \u0026#34;random forest\u0026#34;, \u0026#34;stepwise\u0026#34;, \u0026#34;Elastic\u0026#34;) (tbl.cp) %\u0026gt;% xtable() which.min(err.rf) ```   ","date":1521504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521504000,"objectID":"6778278b18a2dc11ce9063a477d01e92","permalink":"https://hanchao-zhang.github.io/post/data-mining/","publishdate":"2018-03-20T00:00:00Z","relpermalink":"/post/data-mining/","section":"post","summary":"1. Question 1 Jaccard similarity The definition of the Jaccard Similarity matrix is written as follow $$Jac(A,B)=\\frac{|A\\bigcap B|}{|A\\bigcup B|}$$\nHowever, the way of calculating Jaccard Matrix is little different. THe way of computing the matrix is showed as follow","tags":null,"title":"Data Mining and Statistical Learning Final Project","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://hanchao-zhang.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://hanchao-zhang.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"google-site-verification: google1d75d5473f8d5cb8.html","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d3faeec3d7b9224c3a1d8d2dc423fad5","permalink":"https://hanchao-zhang.github.io/google1d75d5473f8d5cb8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/google1d75d5473f8d5cb8/","section":"","summary":"google-site-verification: google1d75d5473f8d5cb8.","tags":null,"title":"","type":"page"},{"authors":["Hong-Gang Ren","Luqman Safdar","Keven Blighe","Nan Huo","Hanchao Zhang","Lanqiu Yao","etc"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://hanchao-zhang.github.io/publication/preprint/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Liver and GI symptoms lead to prolonged disease progression and higher mortality rates in older COVID-19 patients with a history of smoking. These symptoms may be overlooked during the course of illness but appear to be more insidious than respiratory symptoms in COVID-19 patients.","tags":null,"title":"Liver damage, gastrointestinal symptoms and severity of disease in COVID-19","type":"publication"},{"authors":["Rou-Yu Fang","Hanchao Zhang","Xu Li","Yu-Guang Liu","Hanchao Zhang","Qiu-Ning Sun"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"00b68c4d147889a41cefbc279c8478ba","permalink":"https://hanchao-zhang.github.io/publication/jcd2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/jcd2020/","section":"publication","summary":"With application of the regression model between parameters of nasolabial fold wrinkles and age, the effect of rejuvenation treatment can be quantitatively evaluated in terms of age, which has certain reference and promotion value.","tags":null,"title":"Quantitative evaluation of rejuvenation treatment of nasolabial fold wrinkles by regression model and 3D photography","type":"publication"}]